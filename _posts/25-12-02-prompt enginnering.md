---
layout: single
title: "Prompt Engineering ì™„ì „ ê°€ì´ë“œ (Google Â· 2025) â€” í•œêµ­ì–´ ë²ˆì—­ë³¸"
categories: LLM
tags: [Prompt Engineering, Google, Gemini, LLM, AI]
toc: true
author_profile: false
sidebar:
  nav: "docs"
---

# Prompt Engineering ì™„ì „ ê°€ì´ë“œ  
### Google Â· 2025  
### í•œêµ­ì–´ ì „ì²´ ë²ˆì—­ë³¸  
**ì—­ì: parkjongmin-ddam â€” DDAMNOTE ë¸”ë¡œê·¸**

---

> âš¡ **ì´ ë¬¸ì„œëŠ” Googleì´ 2025ë…„ì— ê³µê°œí•œ ê³µì‹ Prompt Engineering Guide(68p)ì˜ ì „ì²´ í•œêµ­ì–´ ë²ˆì—­ë³¸ì…ë‹ˆë‹¤.**  


---

# ğŸ“š Table of Contents
{: .no_toc }

- TOC  
{:toc}

---

# ğŸ§© 1. Introduction

ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì€ í…ìŠ¤íŠ¸Â·ì´ë¯¸ì§€ë¥¼ ì…ë ¥ë°›ì•„ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.  
í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì€ ë‹¨ìˆœí•œ ê¸°ëŠ¥ì´ ì•„ë‹ˆë¼, **ëª¨ë¸ì˜ ì¶œë ¥ì„ ì›í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ìœ ë„í•˜ëŠ” ì„¤ê³„ ê¸°ìˆ **ì…ë‹ˆë‹¤.

ì¢‹ì€ í”„ë¡¬í”„íŠ¸ëŠ” ë‹¤ìŒì˜ ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤:

- ëª¨ë¸ ë° í•™ìŠµ ë°ì´í„°  
- ë‹¨ì–´ ì„ íƒ  
- êµ¬ì¡°/í¬ë§·  
- temperature/top-k ë“± ìƒ˜í”Œë§ ì„¤ì •  
- ì œê³µë˜ëŠ” ë§¥ë½(Context)

ì´ ë¬¸ì„œëŠ” Vertex AIÂ·Gemini ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.

---

# ğŸ§  2. Prompt Engineering ê°œë¡ 

í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì˜ ëª©ì :

- ëª¨ë¸ì´ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì•ˆì •ì ìœ¼ë¡œ ë‹µë³€í•˜ë„ë¡ ìœ ë„  
- í™˜ê°(hallucination) ìµœì†Œí™”  
- API/RAG ì‹œìŠ¤í…œê³¼ ê²°í•©ë˜ëŠ” êµ¬ì¡°í™” ì¶œë ¥(JSON ë“±) ì•ˆì • í™•ë³´  

í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì€ **ë°˜ë³µ ì‹¤í—˜ ê¸°ë°˜ì˜ ì—”ì§€ë‹ˆì–´ë§ ì‘ì—…**ì…ë‹ˆë‹¤.

---

# âš™ï¸ 3. LLM Output Configuration

í”„ë¡¬í”„íŠ¸ë§Œí¼ ì¤‘ìš”í•œ ê²ƒì´ **ëª¨ë¸ì˜ ì¶œë ¥ ì„¤ì •(Output Config)**ì…ë‹ˆë‹¤.

### ì£¼ìš” íŒŒë¼ë¯¸í„°
- **max_tokens** â€” ì¶œë ¥ ê¸¸ì´  
- **temperature** â€” ë¬´ì‘ìœ„ì„±(ì°½ì˜ì„±)  
- **top-k** â€” ìƒìœ„ kê°œ í† í°ë§Œ í›„ë³´  
- **top-p** â€” í™•ë¥  ëˆ„ì  ê¸°ë°˜ í›„ë³´ ì œí•œ  
- **sampling ì „ëµ** â€” ë¬´ì‘ìœ„/ê²°ì •ì  ì„ íƒ ì¡°ì •  

---

# ğŸ”¥ 4. Temperature

| Temperature | íŠ¹ì§• |
|------------|-------|
| 0.0~0.3 | ì•ˆì •ì , ê²°ì •ì  |
| 0.4~0.7 | ì ë‹¹í•œ ì°½ì˜ì„± |
| 0.8~1.0+ | ë” ì°½ì˜ì , ë¶ˆì•ˆì •ì„± ì¦ê°€ |

Temperatureë¥¼ 0ìœ¼ë¡œ ë‘ë©´ **Top-K/Top-PëŠ” ê±°ì˜ ë¬´ì˜ë¯¸**í•´ì§‘ë‹ˆë‹¤.

---

# ğŸ¯ 5. Top-K & Top-P

### âœ” Top-K  
í™•ë¥  ìƒìœ„ Kê°œë§Œ í›„ë³´ë¡œ ì‚¬ìš©.

### âœ” Top-P (Nucleus Sampling)  
í™•ë¥  ëˆ„ì ì´ P ì´í•˜ì¸ í›„ë³´ë§Œ ì‚¬ìš©.

### âœ” ì¶”ì²œê°’  
- ì¼ë°˜ ì‘ì—…: `T=0.2, P=0.95, K=30`  
- ì°½ì˜ì  ìƒì„±: `T=0.9, P=0.99, K=40`  
- ì •í™•ì„± í•„ìˆ˜ ì‘ì—…(ìˆ˜í•™ ë“±): `T=0`

---

# ğŸš¨ 6. Repetition Loop ì˜¤ë¥˜

Sampling ì„¤ì •ì´ ì˜ëª»ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- â€œand and andâ€¦â€  
- â€œthis means that this means thatâ€¦â€  

TemperatureÂ·Top-KÂ·Top-P ì¡°í•©ì„ ì¡°ì ˆí•´ ë°©ì§€í•©ë‹ˆë‹¤.

---

# ğŸ› ï¸ 7. Prompting Techniques (í”„ë¡¬í”„íŠ¸ ê¸°ë²•)

- Zero-shot  
- One-shot / Few-shot  
- System Prompting  
- Role Prompting  
- Contextual Prompting  
- Step-back  
- Chain-of-Thought (CoT)  
- Self-consistency  
- Tree of Thoughts (ToT)  
- ReAct  
- Automatic Prompt Engineering  

ì´ë“¤ì€ ì„œë¡œ ì¡°í•©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

# ğŸ¯ 8. Zero-shot Prompting

ì˜ˆì‹œ ì—†ì´ ì§€ì‹œë§Œìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê¸°ë³¸ ê¸°ë²•ì…ë‹ˆë‹¤.

ë¬¸ì œê°€ ë³µì¡í• ìˆ˜ë¡ Zero-shotì€ ë¶€ì •í™•í•´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

# ğŸ¯ 9. One-shot & Few-shot Prompting

ì˜ˆì‹œë¥¼ 1ê°œ ì œê³µí•˜ë©´ One-shot,  
ì—¬ëŸ¬ ê°œ ì œê³µí•˜ë©´ Few-shotì…ë‹ˆë‹¤.

âœ” ì˜ˆì‹œëŠ” ëª¨ë¸ì˜ íŒ¨í„´ í•™ìŠµ ëŠ¥ë ¥ì„ ë¹„ì•½ì ìœ¼ë¡œ ëŒì–´ì˜¬ë¦½ë‹ˆë‹¤.

Few-shot ì‚¬ìš© ì‹œ ê¶Œì¥:

- 3~5ê°œì˜ ë‹¤ì–‘ì„± ë†’ì€ ì˜ˆì‹œ  
- ì‹¤ìˆ˜ ì—†ëŠ” ê³ í’ˆì§ˆ ì˜ˆì‹œ  
- í´ë˜ìŠ¤ ê· í˜• ìœ ì§€  

---

# âš™ï¸ 10. System / Role / Context Prompting

### ğŸ§© SYSTEM  
ì¶œë ¥ ê·œì¹™Â·í¬ë§·Â·ì§€ì¹¨ì„ ì •ì˜  
ì˜ˆ: â€œí•­ìƒ JSONìœ¼ë¡œ ì¶œë ¥í•˜ë¼â€

### ğŸ§© ROLE  
ëª¨ë¸ì—ê²Œ ì—­í•  ë¶€ì—¬  
ì˜ˆ: â€œë„ˆëŠ” ì‚¬ì´ë²„ ë³´ì•ˆ ì „ë¬¸ê°€ë‹¤â€

### ğŸ§© CONTEXT  
ë°°ê²½Â·ìƒí™© ì •ë³´ ì œê³µ  
ì˜ˆ: â€œì´ ë¬¸ì„œëŠ” ê¸ˆìœµê¸°ê´€ ë‚´ë¶€ ì •ì±…ì´ë‹¤â€

---

# ğŸ§  11. Step-back Prompting

ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ì „ì—  
â€œí•œ ë‹¨ê³„ ì¶”ìƒì ìœ¼ë¡œ ìƒê°í•˜ë„ë¡â€ ì§€ì‹œí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

íš¨ê³¼:

- ë” ë„“ì€ ê´€ì ì—ì„œ reasoning  
- ê°œë… ì´í•´ ê°œì„   
- CoTì™€ ê²°í•© ì‹œ ê°•ë ¥í•´ì§

---

# ğŸ”— 12. Chain-of-Thought(CoT)


ì¤‘ê°„ reasoning ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì‘ì„±í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.

âœ” ë³µì¡í•œ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ í–¥ìƒ  
âœ” ë…¼ë¦¬ì  ì¼ê´€ì„± ì¦ê°€  

ì£¼ì˜: ë„ˆë¬´ ê¸´ CoTëŠ” ì˜¤íˆë ¤ ì˜¤ë¥˜ë¥¼ ì¦ê°€ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

# ğŸ§© 13. Self-consistency

ì—¬ëŸ¬ ê°œì˜ CoTë¥¼ ìƒì„±í•˜ê³   
ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ê²°ë¡ ì„ ì„ íƒí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

íš¨ê³¼:

- ì•ˆì •ì„± ì¦ê°€  
- ì˜¤ë‹µ/í™˜ê° ê°ì†Œ  
- ë³µì¡í•œ ë…¼ë¦¬ ë¬¸ì œì— íŠ¹íˆ ê°•ë ¥

---

# ğŸŒ² 14. Tree of Thoughts (ToT)

ì—¬ëŸ¬ reasoning ê²½ë¡œ(branch)ë¥¼  
íŠ¸ë¦¬ í˜•íƒœë¡œ í™•ì¥í•˜ì—¬ ìµœì ì˜ íë¦„ì„ ì„ íƒí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

ì í•©í•œ ì‘ì—…:

- ì „ëµ ì„¤ê³„  
- ìµœì í™” ë¬¸ì œ  
- ë³µì¡í•œ ì¶”ë¡   

---

# ğŸ¤– 15. ReAct (Reason + Act)

Reasoning + Action(ë„êµ¬ ì‚¬ìš©)ì„ ê²°í•©í•œ ë°©ì‹.

ì‘ë™ íë¦„:

1. Reason  
2. Action(ê²€ìƒ‰, API, ê³„ì‚° ë“±)  
3. Observation  
4. Answer

RAGÂ·ì—ì´ì „íŠ¸ ê¸°ë°˜ ì‹œìŠ¤í…œì˜ í•µì‹¬.

---

# â™»ï¸ 16. Automatic Prompt Engineering (APE)

LLMì´ ìŠ¤ìŠ¤ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±Â·í‰ê°€Â·ì„ íƒí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

ê³¼ì •:

1. í”„ë¡¬í”„íŠ¸ í›„ë³´ ìƒì„±  
2. ëª¨ë¸/í‰ê°€ê¸°ë¡œ í…ŒìŠ¤íŠ¸  
3. ìµœê³  ì„±ëŠ¥ í”„ë¡¬í”„íŠ¸ ì„ íƒ  

---

# ğŸ§‘â€ğŸ’» 17. Code Prompting

LLMì„ ì´ìš©í•œ ì½”ë“œ ê´€ë ¨ ì‘ì—…:

- ì½”ë“œ ìƒì„±  
- ì½”ë“œ ì„¤ëª…  
- ì½”ë“œ ë²ˆì—­  
- ì½”ë“œ ë””ë²„ê¹…/ë¦¬ë·°  

ê° ì‘ì—…ì€ í˜•ì‹, ì–¸ì–´, ì œì•½ ì¡°ê±´ì„ ëª…í™•íˆ ì§€ì •í•´ì•¼ í’ˆì§ˆì´ ë†’ì•„ì§‘ë‹ˆë‹¤.

---

# ğŸ–¼ï¸ 18. Multimodal Prompting

í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ì˜¤ë””ì˜¤ ë“±  
ì—¬ëŸ¬ í˜•íƒœì˜ ì…ë ¥ì„ ê²°í•©í•œ prompting.

ì ìš© ì˜ˆ:

- UI ìŠ¤í¬ë¦°ìƒ· â†’ ì½”ë“œ ìƒì„±  
- ê·¸ë˜í”„/í‘œ í•´ì„  
- ì´ë¯¸ì§€ ìš”ì•½  
- OCR ê¸°ë°˜ ë¶„ì„  

---

# â­ 19. Best Practices (ëª¨ë²” ì‚¬ë¡€)

### âœ” ì˜ˆì‹œ ì œê³µ  
### âœ” ê°„ê²°í•œ ì„¤ê³„  
### âœ” ì¶œë ¥ êµ¬ì¡° ëª…í™•íˆ  
### âœ” ì œì•½ë³´ë‹¤ ì§€ì‹œ ì‚¬ìš©  
### âœ” ìµœëŒ€ í† í° ê¸¸ì´ ì§€ì •  
### âœ” ë³€ìˆ˜ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸  
### âœ” ì…ë ¥ í˜•ì‹ ë‹¤ì–‘í™”  
### âœ” í´ë˜ìŠ¤ ë¶„í¬ ë‹¤ì–‘í™”  
### âœ” ëª¨ë¸ ì—…ë°ì´íŠ¸ ëŒ€ì‘  

---

# ğŸ”§ 20. JSON Repair

Google ëª¨ë¸ì€ JSON ì¶œë ¥ ì‹œ  
ê²½ë¯¸í•œ ì˜¤ë¥˜ê°€ ìˆì–´ë„ ìë™ìœ¼ë¡œ ë³µêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

# ğŸ“ 21. Working with Schemas

ìŠ¤í‚¤ë§ˆë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ë©´:

- êµ¬ì¡°í™” ì¶œë ¥ ì•ˆì •
- íŒŒì‹± ì˜¤ë¥˜ ê°ì†Œ
- hallucination ê°ì†Œ

ì˜ˆ:

```json
{
  "title": "string",
  "summary": "string",
  "score": "number"
}
```

---

# ğŸ“š 22. ì°¸ê³  ë…¼ë¬¸ (Reference Papers)

ì´ ë¬¸ì„œì—ì„œ ì†Œê°œëœ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë²•ë“¤ì€ ë‹¤ìŒì˜ ì£¼ìš” ì—°êµ¬ ë…¼ë¬¸ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.

## ğŸ”— Chain-of-Thought (CoT) Prompting

**ë…¼ë¬¸:** "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
**ì €ì:** Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou (Google Research)
**ì¶œíŒ:** NeurIPS 2022 / arXiv:2201.11903
**ë°œí‘œì¼:** 2022ë…„ 1ì›”

### í•µì‹¬ ë‚´ìš©
- ì¤‘ê°„ ì¶”ë¡  ê³¼ì •(intermediate reasoning steps)ì„ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±í•˜ë„ë¡ ìœ ë„
- Few-shot ì˜ˆì‹œì— ì¶”ë¡  ë‹¨ê³„ë¥¼ í¬í•¨ì‹œí‚¤ëŠ” ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ë³µì¡í•œ ì¶”ë¡  ëŠ¥ë ¥ í–¥ìƒ
- ì‚°ìˆ , ìƒì‹ ì¶”ë¡ , ê¸°í˜¸ ì¶”ë¡  ë“± ë‹¤ì–‘í•œ ì‘ì—…ì—ì„œ ì„±ëŠ¥ ê°œì„  ì…ì¦
- ì¶©ë¶„íˆ í° ì–¸ì–´ ëª¨ë¸ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì¶”ë¡  ëŠ¥ë ¥ì´ ë°œí˜„ë¨ì„ ì¦ëª…

**ğŸ“„ ë…¼ë¬¸ ë§í¬:** [arXiv:2201.11903](https://arxiv.org/abs/2201.11903)

---

## ğŸ§© Self-Consistency

**ë…¼ë¬¸:** "Self-Consistency Improves Chain of Thought Reasoning in Language Models"
**ì €ì:** Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou (Google Research, Brain Team)
**ì¶œíŒ:** ICLR 2023 / arXiv:2203.11171
**ë°œí‘œì¼:** 2022ë…„ 3ì›” (ìµœì¢… ì—…ë°ì´íŠ¸ 2023ë…„ 3ì›”)

### í•µì‹¬ ë‚´ìš©
- CoTì˜ greedy decodingì„ ëŒ€ì²´í•˜ëŠ” ìƒˆë¡œìš´ ë””ì½”ë”© ì „ëµ
- ë‹¤ì–‘í•œ ì¶”ë¡  ê²½ë¡œë¥¼ ìƒ˜í”Œë§í•œ í›„ ê°€ì¥ ì¼ê´€ëœ(consistent) ë‹µë³€ ì„ íƒ
- Majority votingì„ í†µí•œ ìµœì¢… ë‹µë³€ ê²°ì •

### ì„±ëŠ¥ í–¥ìƒ
- GSM8K: +17.9%
- SVAMP: +11.0%
- AQuA: +12.2%
- StrategyQA: +6.4%
- ARC-challenge: +3.9%

**ğŸ“„ ë…¼ë¬¸ ë§í¬:** [arXiv:2203.11171](https://arxiv.org/abs/2203.11171)

---

## ğŸŒ² Tree of Thoughts (ToT)

**ë…¼ë¬¸:** "Tree of Thoughts: Deliberate Problem Solving with Large Language Models"
**ì €ì:** Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan (Princeton University, Google DeepMind)
**ì¶œíŒ:** NeurIPS 2023 / arXiv:2305.10601
**ë°œí‘œì¼:** 2023ë…„ 5ì›”

### í•µì‹¬ ë‚´ìš©
- CoTë¥¼ ì¼ë°˜í™”í•˜ì—¬ ì—¬ëŸ¬ ì¶”ë¡  ê²½ë¡œë¥¼ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ íƒìƒ‰
- ì¤‘ê°„ ë‹¨ê³„ë§ˆë‹¤ ìê¸° í‰ê°€(self-evaluation)ë¥¼ í†µí•´ ë‹¤ìŒ í–‰ë™ ê²°ì •
- Look-aheadì™€ backtrackingì„ í†µí•œ ì „ì—­ì  ì„ íƒ ê°€ëŠ¥
- ì²´ê³„ì ì¸ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì˜ë„ì  íƒìƒ‰(deliberate exploration)

### ì‹¤í—˜ ê²°ê³¼
- Game of 24 ì‘ì—…ì—ì„œ GPT-4 + CoTëŠ” 4% ì„±ê³µë¥ 
- GPT-4 + ToTëŠ” 74% ì„±ê³µë¥  ë‹¬ì„± (18.5ë°° í–¥ìƒ)

**ğŸ“„ ë…¼ë¬¸ ë§í¬:** [arXiv:2305.10601](https://arxiv.org/abs/2305.10601)
**ğŸ’» ì½”ë“œ:** [GitHub - Tree of Thoughts](https://github.com/princeton-nlp/tree-of-thought-llm)

---

## ğŸ¤– ReAct (Reason + Act)

**ë…¼ë¬¸:** "ReAct: Synergizing Reasoning and Acting in Language Models"
**ì €ì:** Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao (Princeton University, Google Research)
**ì¶œíŒ:** ICLR 2023 / arXiv:2210.03629
**ë°œí‘œì¼:** 2022ë…„ 10ì›”

### í•µì‹¬ ë‚´ìš©
- Reasoning traceì™€ task-specific actionì„ êµì°¨(interleave) ìƒì„±
- ì¶”ë¡ ì´ í–‰ë™ ê³„íšì„ ìœ ë„í•˜ê³ , í–‰ë™ì´ ì™¸ë¶€ ì •ë³´ì›ê³¼ ìƒí˜¸ì‘ìš©
- Wikipedia API ë“± ì™¸ë¶€ ì§€ì‹ ë² ì´ìŠ¤ í™œìš©ìœ¼ë¡œ hallucination ê°ì†Œ
- í™˜ê°(hallucination)ê³¼ ì˜¤ë¥˜ ì „íŒŒ ë¬¸ì œ í•´ê²°

### ì‹¤í—˜ ê²°ê³¼
- **Question Answering (HotpotQA)**: CoT ëŒ€ë¹„ hallucination ëŒ€í­ ê°ì†Œ
- **Fact Verification (Fever)**: ì™¸ë¶€ ì§€ì‹ í™œìš©ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
- **ALFWorld (Interactive Decision Making)**: ì ˆëŒ€ ì„±ê³µë¥  +34%
- **WebShop**: ì ˆëŒ€ ì„±ê³µë¥  +10%
- ë‹¨ 1-2ê°œì˜ in-context ì˜ˆì‹œë§Œìœ¼ë¡œ ëª¨ë°© í•™ìŠµÂ·ê°•í™” í•™ìŠµ ë°©ë²• ëŠ¥ê°€

**ğŸ“„ ë…¼ë¬¸ ë§í¬:** [arXiv:2210.03629](https://arxiv.org/abs/2210.03629)
**ğŸ’» ì½”ë“œ:** [GitHub - ReAct](https://github.com/ysymyth/ReAct)
**ğŸŒ í”„ë¡œì íŠ¸:** [ReAct Official Website](https://react-lm.github.io/)

---

## â™»ï¸ Automatic Prompt Engineering (APE)

**ë…¼ë¬¸:** "Large Language Models Are Human-Level Prompt Engineers"
**ì €ì:** Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba (University of Toronto, Vector Institute, University of Waterloo)
**ì¶œíŒ:** ICLR 2023 / arXiv:2211.01910
**ë°œí‘œì¼:** 2022ë…„ 11ì›” (ìµœì¢… ì—…ë°ì´íŠ¸ 2023ë…„ 3ì›”)

### í•µì‹¬ ë‚´ìš©
- í”„ë¡¬í”„íŠ¸ë¥¼ "í”„ë¡œê·¸ë¨"ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ ìë™ ìƒì„± ë° ìµœì í™”
- LLMì´ í›„ë³´ instructionì„ ì œì•ˆí•˜ê³ , ì ìˆ˜ í•¨ìˆ˜ë¡œ ìµœì  ì„ íƒ
- Human-in-the-loop ì—†ì´ ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„±

### ì‹¤í—˜ ê²°ê³¼
- 24ê°œ NLP ì‘ì—… ì¤‘ 19ê°œì—ì„œ ì‚¬ëŒì´ ì‘ì„±í•œ instructionê³¼ ë™ë“±í•˜ê±°ë‚˜ ë” ìš°ìˆ˜í•œ ì„±ëŠ¥
- **íšê¸°ì  ë°œê²¬**: "Let's think step by step" ë³´ë‹¤ ë” ë‚˜ì€ CoT í”„ë¡¬í”„íŠ¸ ìë™ ë°œê²¬
  - MultiArith: 78.7% â†’ 82.0%
  - GSM8K: 40.7% â†’ 43.0%

**ğŸ“„ ë…¼ë¬¸ ë§í¬:** [arXiv:2211.01910](https://arxiv.org/abs/2211.01910)
**ğŸ’» ì½”ë“œ:** [GitHub - Automatic Prompt Engineer](https://github.com/keirp/automatic_prompt_engineer)
**ğŸŒ í”„ë¡œì íŠ¸:** [APE Project Page](https://sites.google.com/view/automatic-prompt-engineer)

---

## ğŸ“– ì¶”ê°€ ê¶Œì¥ ìë£Œ

### ê³µì‹ ê°€ì´ë“œ ë° ë¬¸ì„œ
- [Prompt Engineering Guide](https://www.promptingguide.ai/) - ë‹¤ì–‘í•œ í”„ë¡¬í”„íŒ… ê¸°ë²•ì˜ ì¢…í•© ê°€ì´ë“œ
- [Google Research Blog - Chain of Thought](https://research.google/blog/language-models-perform-reasoning-via-chain-of-thought/)
- [Google Research Blog - ReAct](https://research.google/blog/react-synergizing-reasoning-and-acting-in-language-models/)

### ì—°êµ¬ ë¦¬ì†ŒìŠ¤
- [Chain-of-Thought Papers Collection](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers) - CoT ê´€ë ¨ ë…¼ë¬¸ ëª¨ìŒ
- [Hugging Face Papers](https://huggingface.co/papers) - ìµœì‹  ë…¼ë¬¸ íë ˆì´ì…˜

---

## ğŸ“Š ê¸°ë²•ë³„ ì ìš© ì‹œë‚˜ë¦¬ì˜¤ ìš”ì•½

| ê¸°ë²• | ìµœì  ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ | ì„±ëŠ¥ í–¥ìƒ í­ | ë¹„ìš©/ë³µì¡ë„ |
|------|------------------|-------------|-----------|
| **CoT** | ìˆ˜í•™, ë…¼ë¦¬ ì¶”ë¡ , ë³µì¡í•œ ë¬¸ì œ | ì¤‘ê°„~ë†’ìŒ | ë‚®ìŒ |
| **Self-Consistency** | ì •í™•ë„ê°€ ì¤‘ìš”í•œ ì¶”ë¡  ì‘ì—… | ë†’ìŒ (+10~18%) | ì¤‘ê°„ (ë‹¤ì¤‘ ìƒ˜í”Œë§) |
| **ToT** | ì „ëµ ê²Œì„, ìµœì í™” ë¬¸ì œ | ë§¤ìš° ë†’ìŒ (18ë°°) | ë†’ìŒ (íŠ¸ë¦¬ íƒìƒ‰) |
| **ReAct** | ì •ë³´ ê²€ìƒ‰, ì‚¬ì‹¤ í™•ì¸, ì—ì´ì „íŠ¸ | ë†’ìŒ (+34%) | ì¤‘ê°„ (ì™¸ë¶€ ë„êµ¬ í•„ìš”) |
| **APE** | í”„ë¡¬í”„íŠ¸ ìµœì í™” ìë™í™” | ì¤‘ê°„~ë†’ìŒ | ë†’ìŒ (ì´ˆê¸° ì„¤ì •) |

---

## ğŸ“ ì¸ìš© (Citations)

ì´ ë¬¸ì„œì˜ ë…¼ë¬¸ë“¤ì„ ì¸ìš©í•˜ì‹¤ ê²½ìš° ì•„ë˜ í˜•ì‹ì„ ì‚¬ìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤:

```bibtex
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wang2023self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}

@article{zhou2022large,
  title={Large language models are human-level prompt engineers},
  author={Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
  journal={International Conference on Learning Representations (ICLR)},
  year={2023}
}
```



